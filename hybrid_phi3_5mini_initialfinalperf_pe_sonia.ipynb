{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr7kd_wYEjsy"
      },
      "source": [
        "# Run libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J70AdttPfhc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install -U bitsandbytes\n",
        "%pip install -U transformers\n",
        "%pip install -U accelerate\n",
        "%pip install -U peft\n",
        "%pip install -U trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXgn62pqRr5u"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from google.colab import userdata\n",
        "wb_token = userdata.get('WandB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ2LhR-IR4cA"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    project='Fine-tune-Microsoft-Phi-3.5-mini-instruct-Fixed',\n",
        "    job_type=\"training\",\n",
        "    anonymous=\"allow\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLC2JKY8R-9Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import bitsandbytes as bnb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, PeftConfig\n",
        "from trl import SFTTrainer\n",
        "from trl import setup_chat_format\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                          TrainingArguments,\n",
        "                          pipeline,\n",
        "                          logging)\n",
        "from sklearn.metrics import (accuracy_score,\n",
        "                             classification_report,\n",
        "                             confusion_matrix,\n",
        "                             f1_score)\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0XA3y89Epcr"
      },
      "source": [
        "# With hybrid dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H714CZCdSlrB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/DSSI 2025/Group 1: DSSI Summer 2025/Data/summary_banktrak - summary_banktrak.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irB8wFc1Tzul"
      },
      "outputs": [],
      "source": [
        "# Shuffle\n",
        "df = df.sample(frac=1, random_state=85).reset_index(drop=True)\n",
        "\n",
        "# Split the DataFrame\n",
        "train_size = 0.8\n",
        "eval_size = 0.1 # different from 80/20\n",
        "\n",
        "# Calculate sizes\n",
        "train_end = int(train_size * len(df))\n",
        "eval_end = train_end + int(eval_size * len(df))\n",
        "\n",
        "# Split the data\n",
        "X_train = df[:train_end].copy()  # Use .copy() to avoid warnings\n",
        "X_eval = df[train_end:eval_end].copy()\n",
        "X_test = df[eval_end:].copy()\n",
        "\n",
        "# IMPORTANT: Preserve the original text before generating prompts\n",
        "X_train['original_text'] = X_train['text'].copy()\n",
        "X_eval['original_text'] = X_eval['text'].copy()\n",
        "X_test['original_text'] = X_test['text'].copy()\n",
        "\n",
        "# Define the prompt generation functions - now using 'original_text'\n",
        "# Refined prompt and made it more concise without examples due to previous errors\n",
        "def generate_prompt(data_point):\n",
        "    return f\"\"\"You are a financial text classifier. Answer ONLY \"True\" if the text explicitly mentions debt instruments (bonds, loans, credit agreement, debt settlement, promissory notes).\n",
        "    If the item just references a debt instrument, this column should still be false. There must be some details outside of the name (such as the start date, the amount, the lenders, etc.).\n",
        "    Answer \"False\" for all other topics.\n",
        "\n",
        "Now classify this text:\n",
        "Text: {data_point[\"original_text\"]}\n",
        "Classification: \"\"\".strip()\n",
        "\n",
        "def generate_test_prompt(data_point):\n",
        "    return f\"\"\"\n",
        "            Classify the text into True (debt-related) or False (non-debt-related).\n",
        "Text: {data_point[\"original_text\"]}\n",
        "Classification: \"\"\".strip()\n",
        "\n",
        "# Generate prompts for training and evaluation data - now using original_text\n",
        "X_train['text'] = X_train.apply(generate_prompt, axis=1)\n",
        "X_eval['text'] = X_eval.apply(generate_prompt, axis=1)\n",
        "\n",
        "# Generate test prompts and extract true labels\n",
        "y_true = X_test['contains_debt_instrument_information'].copy()\n",
        "X_test['text'] = X_test.apply(generate_test_prompt, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJavkxJVUbQb"
      },
      "outputs": [],
      "source": [
        "X_train.contains_debt_instrument_information.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOjiiIH8Ujjq"
      },
      "outputs": [],
      "source": [
        "y_true.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skIAEF7VU0Se"
      },
      "outputs": [],
      "source": [
        "# Convert to datasets\n",
        "train_data = Dataset.from_pandas(X_train[[\"text\"]])\n",
        "eval_data = Dataset.from_pandas(X_eval[[\"text\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJ62PyOXU2e-"
      },
      "outputs": [],
      "source": [
        "train_data['text'][3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_CFC2esU6YH"
      },
      "outputs": [],
      "source": [
        "base_model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"float16\",\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qncTwgFQU_Nk"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_s7MjPGPucA"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Memory-optimized predict function\n",
        "def predict(test, model, tokenizer):\n",
        "    import torch\n",
        "    y_pred = []\n",
        "\n",
        "    # Clear GPU cache before starting\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    for i in tqdm(range(len(test))):\n",
        "        prompt = test.iloc[i][\"text\"]\n",
        "\n",
        "        try:\n",
        "            # Use pipeline for text generation with memory optimization\n",
        "            pipe = pipeline(task=\"text-generation\",\n",
        "                            model=model,\n",
        "                            tokenizer=tokenizer,\n",
        "                            max_new_tokens=5, # maybe change to 1 or 2\n",
        "                            temperature=0.1,\n",
        "                            device_map=\"auto\",\n",
        "                            torch_dtype=torch.float16)  # Use half precision\n",
        "\n",
        "            result = pipe(prompt)\n",
        "            response = result[0]['generated_text']\n",
        "            prediction = response.strip().split(\"Response:\")[-1].strip()\n",
        "            # Parse the binary classification result\n",
        "            if \"true\" in prediction.lower():\n",
        "                y_pred.append(True)\n",
        "            elif \"false\" in prediction.lower():\n",
        "                y_pred.append(False)\n",
        "            else:\n",
        "                # Default to False if unclear\n",
        "                y_pred.append(False)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sample {i}: {e}\")\n",
        "            y_pred.append(False)  # Default on error\n",
        "\n",
        "        # Clear cache every 10 samples to prevent memory buildup\n",
        "        if i % 10 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFCX452gdJW9"
      },
      "outputs": [],
      "source": [
        "# predict(X_test, model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IlhwSEVVTtt"
      },
      "outputs": [],
      "source": [
        "y_pred = predict(X_test, model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgxh8j0XVU8A"
      },
      "outputs": [],
      "source": [
        "def evaluate(y_true, y_pred):\n",
        "    labels = [False, True]  # Binary labels for debt classification\n",
        "    label_names = [\"Non-debt-related\", \"Debt-related\"]\n",
        "\n",
        "    # Convert y_true to a list to allow integer indexing\n",
        "    y_true_list = y_true.tolist()\n",
        "\n",
        "    # Calculate accuracy and F1 score\n",
        "    accuracy = accuracy_score(y_true=y_true_list, y_pred=y_pred)\n",
        "    f1 = f1_score(y_true=y_true_list, y_pred=y_pred)\n",
        "    print(f'Accuracy: {accuracy:.3f}')\n",
        "    print(f'F1 Score: {f1:.3f}')\n",
        "\n",
        "    # Generate accuracy report for each class\n",
        "    unique_labels = set(y_true_list)  # Get unique labels from the list\n",
        "\n",
        "    for label in unique_labels:\n",
        "        label_indices = [i for i in range(len(y_true_list)) if y_true_list[i] == label]\n",
        "        label_y_true = [y_true_list[i] for i in label_indices]\n",
        "        label_y_pred = [y_pred[i] for i in label_indices]\n",
        "        label_accuracy = accuracy_score(label_y_true, label_y_pred)\n",
        "        label_name = \"Debt-related\" if label else \"Non-debt-related\"\n",
        "        print(f'Accuracy for {label_name}: {label_accuracy:.3f}')\n",
        "\n",
        "    # Generate classification report\n",
        "    class_report = classification_report(y_true=y_true_list, y_pred=y_pred, target_names=label_names)\n",
        "    print('\\nClassification Report:')\n",
        "    print(class_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oja9LuyzVaoJ"
      },
      "outputs": [],
      "source": [
        "evaluate(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGjh7bZVaMGG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=['False', 'True'])\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title('Phi Model Performance on Original Dataset')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLQnW9R2eyLF"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Memory-optimized predict function\n",
        "def predict(test, model, tokenizer):\n",
        "    import torch\n",
        "    y_pred = []\n",
        "\n",
        "    # Clear GPU cache before starting\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    for i in tqdm(range(len(test))):\n",
        "        prompt = test.iloc[i][\"text\"]\n",
        "\n",
        "        try:\n",
        "            # Use pipeline for text generation with memory optimization\n",
        "            pipe = pipeline(task=\"text-generation\",\n",
        "                            model=model,\n",
        "                            tokenizer=tokenizer,\n",
        "                            max_new_tokens=2,\n",
        "                            temperature=0.1,\n",
        "                            device_map=\"auto\",\n",
        "                            torch_dtype=torch.float16)  # Use half precision\n",
        "\n",
        "            result = pipe(prompt)\n",
        "            response = result[0]['generated_text']\n",
        "            prediction = response.strip().split(\"Response:\")[-1].strip()\n",
        "\n",
        "            # Parse the binary classification result\n",
        "            if \"true\" in prediction.lower():\n",
        "                y_pred.append(True)\n",
        "            elif \"false\" in prediction.lower():\n",
        "                y_pred.append(False)\n",
        "            else:\n",
        "                # Default to False if unclear\n",
        "                y_pred.append(False)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sample {i}: {e}\")\n",
        "            y_pred.append(False)  # Default on error\n",
        "\n",
        "        # Clear cache every 10 samples to prevent memory buildup\n",
        "        if i % 10 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset = load_dataset(\"csv\", data_files=\"/content/gdrive/MyDrive/DSSI 2025/Group 1: DSSI Summer 2025/Data/test.csv\", split=\"train\")\n",
        "\n",
        "# Convert to pandas DataFrame for easier manipulation\n",
        "test_df = test_dataset.to_pandas()\n",
        "\n",
        "# Generate test prompts (without labels)\n",
        "def generate_test_prompt(data_point):\n",
        "    return f\"\"\"\n",
        "            Classify the text into True (debt-related) or False (non-debt-related).\n",
        "text: {data_point[\"text\"]}\n",
        "label: \"\"\".strip()\n",
        "\n",
        "# Extract true labels and create test prompts\n",
        "y_true_test = test_df[\"contains_debt_instrument_information\"]\n",
        "test_df_prompts = pd.DataFrame(test_df.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n",
        "\n",
        "# Set environment variable for memory optimization\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# Run predictions with memory optimization\n",
        "y_pred_test = predict(test_df_prompts, model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "275I0nYvg3EF"
      },
      "outputs": [],
      "source": [
        "evaluate(y_true_test, y_pred_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K8ES7jLgR8r"
      },
      "outputs": [],
      "source": [
        "# Create confusion matrix with increased font sizes\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_true_list = y_true_test.tolist()\n",
        "cm = confusion_matrix(y_true=y_true_list, y_pred=y_pred_test)\n",
        "\n",
        "# Set larger font sizes\n",
        "plt.rcParams.update({'font.size': 14})  # Base font size\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=['Non-debt-related', 'Debt-related'])\n",
        "\n",
        "# Create the plot with adjusted figure size for better spacing\n",
        "fig, ax = plt.subplots(figsize=(9, 7))\n",
        "disp.plot(cmap='Blues', ax=ax)\n",
        "\n",
        "# Customize font sizes for different elements\n",
        "plt.title('Phi Model Performance on Newly Annotated Dataset', fontsize=20, pad=20)\n",
        "ax.set_xlabel('Predicted Label', fontsize=18)\n",
        "ax.set_ylabel('True Label', fontsize=18)\n",
        "\n",
        "# Increase tick label font size\n",
        "ax.tick_params(axis='both', which='major', labelsize=16)\n",
        "\n",
        "# Keep matrix cell numbers at default size (no change needed)\n",
        "\n",
        "plt.tight_layout(pad=3.0)  # Increase padding further\n",
        "plt.show()\n",
        "\n",
        "# Reset font settings to default after plotting (optional)\n",
        "plt.rcParams.update({'font.size': 10})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A31yjLU7csi_"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "new_model_local = \"stpereir-Phi-3.5-summarized-and-test\"\n",
        "model.save_pretrained(new_model_local)\n",
        "tokenizer.save_pretrained(new_model_local)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}