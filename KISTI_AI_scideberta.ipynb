{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlYoLsRwrwZj"
      },
      "source": [
        "**Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z5iJlJS-nLbf",
        "outputId": "fe190e23-8640-4cb4-9f44-a1094557f43a"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "mount failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1266228688.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!pip install transformers\n",
        "!pip install -U datasets\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "import json\n",
        "import torch\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l8XfQwXvpZ0M"
      },
      "outputs": [],
      "source": [
        "#banktrak_df = pd.read_csv('/content/gdrive/MyDrive/Group 1: DSSI Summer 2025/Data/summary_banktrak.csv')\n",
        "#banktrak_df_1 = pd.read_csv('/content/gdrive/MyDrive/Group 1: DSSI Summer 2025/Data/banktrak-8K-20230501-annotated.csv')\n",
        "#banktrak_df.head(10)\n",
        "model_path = \"/content/gdrive/MyDrive/DSSI/distilbert-model\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiUekEjCbUJd"
      },
      "source": [
        "1. USING SAMPLERS FOR CLASS IMBALANCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aYSdYVuH5zxS"
      },
      "outputs": [],
      "source": [
        "# #https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html#imblearn.over_sampling.RandomOverSampler\n",
        "# from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# X = banktrak_df_1[['text']]\n",
        "# y = banktrak_df_1[['contains_debt_instrument_information']]\n",
        "\n",
        "# #oversampling minority data\n",
        "# ros = RandomOverSampler(random_state=42)                 #this is the random oversampling\n",
        "# X_resampled, y_resampled = ros.fit_resample(X, y)        #i think this is fitting our model with the undersampling\n",
        "# y_resampled.contains_debt_instrument_information.value_counts()\n",
        "\n",
        "# X_resampled = X_resampled.reset_index(drop=True)\n",
        "# y_resampled = y_resampled.reset_index(drop=True)\n",
        "\n",
        "# banktrak_df = pd.concat([X_resampled, y_resampled], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lMvSLKWxhtoM"
      },
      "outputs": [],
      "source": [
        "# #https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n",
        "# from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# X = banktrak_df_1[['text']]\n",
        "# y = banktrak_df_1[['contains_debt_instrument_information']]\n",
        "\n",
        "# #oversampling minority data\n",
        "# rus = RandomUnderSampler(random_state=42)                 #this is the random oversampling\n",
        "# X_resampled, y_resampled = rus.fit_resample(X, y)        #i think this is fitting our model with the undersampling\n",
        "# y_resampled.contains_debt_instrument_information.value_counts()\n",
        "\n",
        "# X_resampled = X_resampled.reset_index(drop=True)\n",
        "# y_resampled = y_resampled.reset_index(drop=True)\n",
        "\n",
        "# banktrak_df = pd.concat([X_resampled, y_resampled], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS72jS6RtY7D"
      },
      "source": [
        "2. SPLIT INTO TRAINING AND VALIDATION THEN TURNING INTO DICTIONARY\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3xXTq4-jpwQX"
      },
      "outputs": [],
      "source": [
        "#spliting data train and val\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(banktrak_df, test_size=0.2, random_state=42) #validation size is 20%, random state is just there for reproducilbility\n",
        "\n",
        "# turn to json dict\n",
        "from datasets import Dataset, DatasetDict\n",
        "dataset = DatasetDict({'train': Dataset.from_pandas(train_df), 'validation': Dataset.from_pandas(val_df)}) #use dataset dict to turn to dataframe to dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T0iMHB_bFeB"
      },
      "source": [
        "3. Loading the model & tokenizer in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pHgeMNItnrXS"
      },
      "outputs": [],
      "source": [
        "# load model and tokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"KISTI-AI/scideberta\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"KISTI-AI/scideberta\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLZml7a7sA8i"
      },
      "source": [
        "**Dataset Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-dTvayO-p5xX"
      },
      "outputs": [],
      "source": [
        "# apply tokenizer to dataset\n",
        "tokenized_dataset = dataset.map(lambda example: tokenizer(example['text'], padding=\"max_length\", truncation=True, max_length=64))\n",
        "\n",
        "#clean dataset\n",
        "#tokenized_dataset = tokenized_dataset.remove_columns(['item', 'text', 'company','cik'])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"contains_debt_instrument_information\", \"labels\") #rename this column labels bz the model likes that\n",
        "tokenized_dataset.set_format(\"torch\")\n",
        "#turn to pytorch tensor for model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3X1Nx78syny"
      },
      "source": [
        "**Using DataLoader to batchify data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GXxtA1dTqGpv"
      },
      "outputs": [],
      "source": [
        "#call train and eval\n",
        "train_dataset = tokenized_dataset['train'].shuffle(seed=1111)\n",
        "eval_dataset = tokenized_dataset['validation']\n",
        "\n",
        "#apply dataloader to train and eval dataloader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=8, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RljC0a_s3oP"
      },
      "source": [
        "**Training and Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_DhhhVkZqHz7"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import set_seed\n",
        "from torch.optim import AdamW\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"KISTI-AI/scideberta\", num_labels=2).to(device)\n",
        "\n",
        "num_epochs = 1\n",
        "num_training_steps = len(train_dataloader)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01) #2e-5 to 5e-5\n",
        "\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "best_val_loss = float(\"inf\") # starts at infilty so any real loss will be smaller\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "#tqdm tracks iteration status\n",
        "\n",
        "for epoch in range(num_epochs): #for num of epochs\n",
        "    # training\n",
        "    model.train() # training mode\n",
        "    training_losses = [] #store batch losses\n",
        "    for batch_i, batch in enumerate(train_dataloader): #for batches in training loader\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # copy input to device\n",
        "        input_ids = batch['input_ids'].to(device)            #moves these to same device\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        labels= labels.long()\n",
        "\n",
        "\n",
        "        # Call the model for Forward Pass\n",
        "        output = model(input_ids = input_ids, attention_mask = attention_mask, labels =labels)\n",
        "        training_loss = output.loss                                           #takes loss data\n",
        "        training_losses.append(training_loss.item())                          #adds loss data\n",
        "\n",
        "        #Do backprop and update params by taking an optimization step\n",
        "        training_loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()                                                   #pdates learning rate according to scheduler policy (linear decay here)\n",
        "        progress_bar.update(1)                                                #advances the tqdm progress bar by 1 step (one batch done)\n",
        "    print(\"Mean Training Loss\", np.mean(training_losses))\n",
        "\n",
        "    # validation\n",
        "    val_loss = 0\n",
        "    #set to evaluation mode because we dont want to collect gradients\n",
        "    model.eval()            #disables things like dropout and layer norm randomness\n",
        "    for batch_i, batch in enumerate(eval_dataloader):                  #go over batches\n",
        "        with torch.no_grad():                                          #clear gradients\n",
        "            # copy input to device\n",
        "            input_ids = batch['input_ids'].to(device)                  #add to device\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            labels = labels.long()\n",
        "\n",
        "            #call the model again for Forward Pass\n",
        "            output = model(input_ids = input_ids, attention_mask = attention_mask, labels =labels)\n",
        "\n",
        "        # add the batch average of validation loss to the running sum\n",
        "        val_loss += output.loss               # batch’s loss to the total validation loss.\n",
        "\n",
        "    # calculating average validation loss across all batches\n",
        "    avg_val_loss = val_loss / len(eval_dataloader)                               #calculations of best loss\n",
        "    print(f\"Validation loss: {avg_val_loss}\")\n",
        "\n",
        "    # Saving this model checkpoint only if the current validation loss\n",
        "    # is better than the best validation loss obtained so far\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        print(\"Saving checkpoint!\")\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),'val_loss': best_val_loss,}, f\"{model_path}epoch_{epoch}.pt\")\n",
        "    print()\n",
        "\n",
        "print(f\"The best validation loss after {num_epochs} epochs is: {best_val_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvJBTnUktC9_"
      },
      "source": [
        "**Evaluate your model on Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uR8019boqQq_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "eval_dataloader = DataLoader(tokenized_dataset['validation'], batch_size=len(tokenized_dataset['validation']))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.eval()            #evalaution mode\n",
        "test_batch_logits = []    #store models raw outputs logits\n",
        "y_true = []\n",
        "for batch_i, batch in enumerate(eval_dataloader): #go through batches\n",
        "    with torch.no_grad():                          #clear graidnet\n",
        "        # copy input to device\n",
        "        input_ids = batch['input_ids'].to(device)            #add to deviice\n",
        "        attention_mask = batch['attention_mask'].to(device)  #add device\n",
        "        labels = batch['labels'].cpu().detach().numpy()      #removes labels\n",
        "\n",
        "        # Call the model on test data\n",
        "        output = model(input_ids = input_ids, attention_mask = attention_mask, labels =None) #labels none becuase didnt pass any\n",
        "        test_batch_logits.append(output.logits)                #append output logits --stores logits before soft max\n",
        "        y_true.extend(labels)                                  #add ground turth labels\n",
        "\n",
        "print(len(test_batch_logits),len(eval_dataloader))      #shape of the final logits tensor ([num_examples, num_classes])\n",
        "test_logits = torch.cat(test_batch_logits, dim=0)     #concatenates the logits from all batches along dimension 0.\n",
        "\n",
        "#sanity check -> dimension 0 of your logits tensor should be same as the size of the test dataset\n",
        "print(test_logits.shape,len(tokenized_dataset['validation']),len(y_true))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JCxAltj9qWEU"
      },
      "outputs": [],
      "source": [
        "#Convert the logits to predicted labels\n",
        "y_pred = torch.argmax(test_logits, dim = 1).cpu().numpy()\n",
        "\n",
        "print(y_true[:10])\n",
        "print(y_pred[:10])\n",
        "\n",
        "#sanity check: should have as many predictions as labels\n",
        "assert len(y_pred)==len(y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r-MxYcfLqbEx"
      },
      "outputs": [],
      "source": [
        "# call the f1_score function\n",
        "print('F1 Score:',f1_score(y_true, y_pred, average='binary'))\n",
        "\n",
        "# call the accuracy_score function\n",
        "print('Accuracy Score:',accuracy_score(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMR06w6foBml"
      },
      "source": [
        "new stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oX_L2vZDtr3G"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VhmhQ_fyinE3"
      },
      "outputs": [],
      "source": [
        "# pip install transformer-ranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YNq4yyZTihwA"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "# from transformer_ranker import TransformerRanker, prepare_popular_models\n",
        "\n",
        "# # Step 1: Load the CoNLL-03 dataset from HuggingFace\n",
        "# dataset = load_dataset('conll2003')\n",
        "\n",
        "# # Step 2: Use our list of 17 'base' LMs as candidates\n",
        "# language_models = prepare_popular_models('base')\n",
        "\n",
        "# # Step 3: Initialize the ranker with the dataset\n",
        "# ranker = TransformerRanker(dataset, dataset_downsample=0.2)\n",
        "\n",
        "# # ... and run the ranker to obtain the ranking\n",
        "# results = ranker.run(language_models, batch_size=64)\n",
        "\n",
        "# # print the ranking\n",
        "# print(results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}